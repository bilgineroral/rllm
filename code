./collect_code.sh
#!/bin/bash

# Check if a GitHub URL is provided as an argument
# Store the GitHub URL
> code

# Define an array of popular code file extensions
EXTENSIONS=("py" "js" "ts" "jsx" "tsx" "rs" "ex" "exs" "go" "java" "c" "cpp" "h" "hpp" \
"cs" "rb" "php" "html" "css" "kt" "swift" "scala" "sh" "pl" "r" "lua" "m" "erl" "hs")

# Build the find command arguments to search for files with the specified extensions
FIND_ARGS=()
for EXT in "${EXTENSIONS[@]}"; do
    FIND_ARGS+=( -iname "*.$EXT" -o )
done
# Remove the last '-o' (logical OR) operator
unset 'FIND_ARGS[${#FIND_ARGS[@]}-1]'

# Find all files matching the extensions and process them
find . -type f \( "${FIND_ARGS[@]}" \) | while read -r FILE; do
    # Append the filename to the 'code' file
    echo "$FILE" >> code
    # Append the file content to the 'code' file
    cat "$FILE" >> code
done
./tokenizer.py
import torch

class RNATokenizer:
    def __init__(self):
        """
        Initializes the RNA tokenizer with token-to-index and index-to-token mappings.
        """
        self.vocab = {
            "<CLS>": 0,    # Start token
            "<PAD>": 1,    # Padding token
            "<EOS>": 2,    # End token
            "<UNK>": 3,    # Unknown token
            "A": 5, "a": 5,
            "C": 7, "c": 7,
            "G": 4, "g": 4,
            "U": 6, "u": 6,
            "T": 6, "t": 6
        }
        self.vocab_size = len(set(self.vocab.values()))
        self.cls_token = 0  # <CLS>
        self.pad_token = 1  # <PAD>
        self.eos_token = 2  # <EOS>
        self.unk_token = 3  # <UNK>

    def __call__(self, sequence: str):
        """
        Tokenizes a single RNA sequence into token indices.

        Args:
            sequence (str): RNA sequence (e.g., "cggccu").

        Returns:
            tokens (list): Tokenized sequence with CLS and EOS tokens added.
        """
        tokens = [self.cls_token]  # Add CLS token
        for char in sequence:
            tokens.append(self.vocab.get(char, self.unk_token))  # Map character to token or use UNK
        tokens.append(self.eos_token)  # Add EOS token
        return tokens

    def pad_sequences(self, sequences: list, pad_value: int = None):
        """
        Pads a list of tokenized sequences to the same length.

        Args:
            sequences (list of list): List of tokenized RNA sequences.
            pad_value (int): Padding value. Defaults to tokenizer's pad_token (1).

        Returns:
            padded_sequences (torch.Tensor): Padded tensor of shape [batch_size, max_seq_len].
            lengths (list): List of original sequence lengths before padding.
        """
        if pad_value is None:
            pad_value = self.pad_token

        lengths = [len(seq) for seq in sequences]
        max_len = max(lengths)
        padded_sequences = torch.full((len(sequences), max_len), pad_value, dtype=torch.long)

        for i, seq in enumerate(sequences):
            padded_sequences[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)

        return padded_sequences, lengths
./model.py
import torch
import torch.nn as nn
import einops

class CrossAttentionBlock(nn.Module):
    def __init__(self, d_protein: int = 1536, 
                 d_rna: int = 768, 
                 d_model: int = 768, num_heads: int = 8
                ):
        """
        d_protein: Dimension of protein embeddings
        d_rna: Dimension of RNA embeddings
        d_model: Dimension of model embeddings
        num_heads: Number of attention heads
        vocab_size: Number of tokens in the output vocabulary
        """
        super().__init__()
        self.protein_proj = nn.Linear(d_protein, d_model)
        self.rna_proj = nn.Linear(d_rna, d_model)
        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, protein_emb: torch.Tensor, 
                rna_emb: torch.Tensor, 
                protein_mask: torch.Tensor = None, 
                rna_mask: torch.Tensor = None) -> torch.Tensor:
        """
        protein_emb: [B, prot_len, d_protein]
        rna_emb: [B, rna_len, d_rna]
        protein_mask: [B, prot_len]
        rna_mask: [B, rna_len]
        """
        # Project protein and RNA embeddings to a common dimension
        protein_proj = self.protein_proj(protein_emb)  # [B, prot_len, d_model]
        rna_proj = self.rna_proj(rna_emb)              # [B, rna_len, d_model]

        # Cross Attention: RNA queries protein
        attn_output, _ = self.cross_attention(
            query=rna_proj, key=protein_proj, value=protein_proj,
            key_padding_mask=protein_mask
        )

        # Mask out padded RNA positions
        if rna_mask is not None:
            rna_mask = rna_mask.unsqueeze(-1)  # [B, rna_len, 1]
            attn_output = attn_output.masked_fill(rna_mask, 0.0)

        # Add & Norm
        output = self.layer_norm(rna_proj + attn_output)
        return output


class ParallelizedCrossAttentionModel(nn.Module):
    def __init__(self, d_rna: int = 768,
                 d_protein: int = 1536,
                 d_model: int = 768,
                 num_heads: int = 8,
                 num_layers: int = 12,
                 vocab_size: int = 8):
        super().__init__()
        self.num_layers = num_layers
        self.cross_attention_blocks = nn.ModuleList([
            CrossAttentionBlock(d_protein, d_rna, d_model, num_heads) for _ in range(num_layers)
        ])
        self.output_head = nn.Linear(d_model * num_layers, vocab_size)  # Final projection to vocabulary size

    def forward(self, rna_emb: torch.Tensor, 
                protein_emb: torch.Tensor, 
                rna_mask: torch.Tensor = None,
                protein_mask: torch.Tensor = None) -> torch.Tensor:
        """
        rna_emb: [batch_size, num_layers, rna_seq_len, d_rna]
        protein_emb: [batch_size, protein_seq_len, d_protein]
        protein_mask: [batch_size, protein_seq_len]
        rna_mask: [batch_size, rna_seq_len]
        """
        num_layers = rna_emb.shape[1]
        assert num_layers == self.num_layers, "Mismatch in number of layers"

        # Fork each cross-attention block
        futures = []
        for i in range(self.num_layers):
            rna_layer_emb = rna_emb[:, i, :, :]  # Shape: [batch_size, rna_seq_len, d_rna]
            futures.append(torch.jit.fork(
                self.cross_attention_blocks[i],
                protein_emb, rna_layer_emb, protein_mask, rna_mask
            ))

        # Gather results
        outputs = [torch.jit.wait(fut) for fut in futures]

        # Stack outputs along the layer dimension
        concatenated_output = torch.stack(outputs, dim=1)  # Shape: [batch_size, num_layers, rna_seq_len, d_model]
        concatenated_output = einops.rearrange(concatenated_output, 'b l r d -> b r (l d)')

        logits = self.output_head(concatenated_output)  # Shape: [B, rna_seq_len, vocab_size]
        return logits./train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from functools import partial

from model import ParallelizedCrossAttentionModel
from dataset import ProteinRNADataset, collate_fn
from tokenizer import RNATokenizer

# Hyperparameters
batch_size = 2
num_layers = 12
d_rna = 768
d_protein = 1536
d_model = 768
num_heads = 8
vocab_size = 8
num_epochs = 10
learning_rate = 1e-4

protein_data_path = "/home/bil/dataset/embeddings/protein"
rna_data_path = "/home/bil/dataset/embeddings/rna"
pairs_data_path = "/home/bil/dataset/embeddings/pairs.txt"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


tokenizer = RNATokenizer()

# DataLoader
dataset = ProteinRNADataset(
    pairs_file=pairs_data_path,
    protein_folder=protein_data_path, 
    rna_folder=rna_data_path,
    tokenizer=tokenizer
)
dataloader = DataLoader(
    dataset, 
    batch_size=batch_size, 
    collate_fn=partial(collate_fn, tokenizer=tokenizer)
)

# Model, Loss, Optimizer
model = ParallelizedCrossAttentionModel(
    d_rna=d_rna, d_protein=d_protein, d_model=d_model,
    num_heads=num_heads, num_layers=num_layers, vocab_size=vocab_size
).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token) 

# Training Loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for sample in dataloader:
        protein_batch, rna_batch, protein_mask, rna_mask, target_batch = (
            sample["protein"].to(device), 
            sample["rna"].to(device), 
            sample["protein_mask"].to(device), 
            sample["rna_mask"].to(device), 
            sample["rna_targets"].to(device)
        )
        optimizer.zero_grad()
        
        # Forward pass
        logits = model(
            rna_batch,
            protein_batch,
            rna_mask,
            protein_mask
        ) # [B, seq_len, vocab_size]
        
        # Reshape for loss computation
        logits = logits.view(-1, vocab_size) # [B * seq_len, vocab_size]
        targets = target_batch.view(-1)      # [B * seq_len]
        
        # Compute loss
        loss = criterion(logits, targets)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        print(loss.item())
    
    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}")

print("Training complete!")./dataset.py
import torch
import os
from torch.utils.data import Dataset

from tokenizer import RNATokenizer

class ProteinRNADataset(Dataset):
    def __init__(self, pairs_file: str, 
                 protein_folder: str, 
                 rna_folder: str, 
                 tokenizer: RNATokenizer = None):
        """
        Args:
            pairs_file: Path to 'representative_pairs.txt'
            protein_folder: Path to protein embeddings
            rna_folder: Path to RNA embeddings
            tokenizer: Function that tokenizes RNA sequences into indices
        """
        self.protein_folder = protein_folder
        self.rna_folder = rna_folder
        if tokenizer is None:
            tokenizer = RNATokenizer()
        self.tokenizer = tokenizer

        # Parse the pairs file
        self.pairs = []
        with open(pairs_file, 'r') as f:
            lines = f.readlines()
            assert len(lines) % 2 == 0, "Invalid pairs file"
            for i in range(0, len(lines), 2):  # Every two lines
                identifier = lines[i].strip()[1:]
                rna_seq = lines[i+1].strip().split('$')[1]
                pdb_id, prot_chain, rna_chain = identifier.split('_')

                self.pairs.append({
                    "pdb_id": pdb_id,
                    "prot_chain": prot_chain,
                    "rna_chain": rna_chain, 
                    "rna_seq": rna_seq
                })

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        # Retrieve file paths
        pair = self.pairs[idx]
        protein_file = os.path.join(self.protein_folder, f"{pair['pdb_id']}_{pair['prot_chain']}.pt")
        rna_file = os.path.join(self.rna_folder, f"{pair['pdb_id']}_{pair['rna_chain']}.pt")

        # Load embeddings lazily
        protein_emb = torch.load(protein_file).squeeze(0)  # Shape: [prot_len + 2, 1536]
        rna_emb = torch.load(rna_file)
        rna_emb = torch.tensor(rna_emb, dtype=torch.float32)  # Shape: [rna_num_layers (12), rna_len + 2, 768]
        
        # Remove the last token (EOS) from RNA embeddings
        rna_emb = rna_emb[:, :-1, :]  # Shape: [12, seq_len + 1, d_rna]

        # Tokenize RNA sequence and create target
        rna_tokens = self.tokenizer(pair["rna_seq"])  # List of token indices with <START> and <END>
        rna_input = torch.tensor(rna_tokens[:-1], dtype=torch.long)  # Input
        rna_target = torch.tensor(rna_tokens[1:], dtype=torch.long) # Target (shifted)

        return {
            "protein": protein_emb,
            "rna": rna_emb, # unused when training RNA model
            "rna_input": rna_input, # unused when using pre-generated embeddings
            "rna_target": rna_target
        }
    

def collate_fn(batch, tokenizer=None, device: torch.device = torch.device('cuda')):
    protein_lens = [item['protein'].shape[0] for item in batch]
    max_protein_len = max(protein_lens)

    # pad protein embeddings with zeros
    protein_padded = torch.stack([
        torch.cat([item['protein'].to(device), torch.zeros(max_protein_len - item['protein'].shape[0], item['protein'].shape[1]).to(device)])
        for item in batch
    ]) # Shape: [batch_size, max_protein_len, d_protein]

    rna_lens = [item['rna'].shape[1] for item in batch]
    max_rna_len = max(rna_lens)

    # pad rna embeddings with zeros
    rna_padded = torch.stack([
        torch.cat([item['rna'].to(device), torch.zeros(item['rna'].shape[0], max_rna_len - item['rna'].shape[1], item['rna'].shape[2]).to(device)], dim=1)
        for item in batch
    ])  # Shape: [batch_size, rna_num_layers (12), max_rna_len, d_rna]

    # Target padding
    if tokenizer is None:
        tokenizer = RNATokenizer()
    rna_targets = [item['rna_target'] for item in batch]
    rna_targets_padded, _ = tokenizer.pad_sequences(rna_targets)

    # Create masks
    protein_padding_mask = torch.tensor([[False] * l + [True] * (max_protein_len - l) for l in protein_lens]).to(device)
    rna_padding_mask = torch.tensor([[False] * l + [True] * (max_rna_len - l) for l in rna_lens]).to(device)

    return {
        "protein": protein_padded,          # [batch_size, max_protein_len, d_protein]
        "rna": rna_padded,                  # [batch_size, max_rna_len, d_rna]
        "protein_mask": protein_padding_mask,  # [batch_size, max_protein_len]
        "rna_mask": rna_padding_mask,          # [batch_size, max_rna_len]
        "rna_targets": rna_targets_padded   # [batch_size, max_rna_len]
    }
